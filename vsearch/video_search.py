#!/usr/bin/env python3
"""video_search.py  â€¢  2025-08-14 (ULTRA-ENHANCED)

Advanced incremental upgrade with performance, security, and reliability improvements.

Key Enhancements:
â€¢ Advanced caching system with TTL support
â€¢ Connection pool management with auto-cleanup
â€¢ Enhanced error recovery with exponential backoff
â€¢ Comprehensive input validation and sanitization
â€¢ Resource monitoring and automatic optimization
â€¢ Improved async operations with better concurrency control
â€¢ Advanced logging with rotation and compression
â€¢ Smart rate limiting per domain
â€¢ Session persistence for interrupted operations
â€¢ Enhanced thumbnail management with format conversion
â€¢ Better memory management and garbage collection
â€¢ Comprehensive metrics and performance tracking
"""

from __future__ import annotations

SCRIPT_VERSION = "2025-08-14-ULTRA"

# â”€â”€ standard libs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import asyncio
import base64
import html
import io
import logging
import logging.handlers
import os
import pickle
import random
import re
import sqlite3
import sys
import tempfile
import threading
import time
import unicodedata
import uuid
from collections import defaultdict, deque
from collections.abc import AsyncGenerator, Iterator, Sequence
from contextlib import asynccontextmanager, contextmanager, suppress
from dataclasses import dataclass
from datetime import datetime
from enum import Enum, auto
from pathlib import Path
from typing import (
    Any,
    NamedTuple,
    TypeVar,
)
from urllib.parse import urlparse

# â”€â”€ third-party â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import requests
from colorama import Fore, Style, init
from requests.adapters import HTTPAdapter, Retry

# Optional async & selenium support
try:
    import aiofiles
    import aiohttp
    from aiohttp import ClientTimeout, TCPConnector
    from aiohttp_retry import ExponentialRetry, RetryClient
    ASYNC_AVAILABLE = True
except ImportError:
    ASYNC_AVAILABLE = False

try:
    from selenium import webdriver
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.support.ui import WebDriverWait
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ModuleNotFoundError:
    TQDM_AVAILABLE = False
    def tqdm(iterable, **kwargs):
        return iterable

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# â”€â”€ Type definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T = TypeVar('T')
ResultDict = dict[str, Any]
HeadersDict = dict[str, str]
ProxyDict = dict[str, str]

class SearchType(Enum):
    VIDEO = auto()
    GIF = auto()
    BOTH = auto()

class OutputFormat(Enum):
    HTML = "html"
    JSON = "json"
    CSV = "csv"
    XML = "xml"

@dataclass
class SearchConfig:
    """Configuration for search operations."""
    query: str
    engine: str
    limit: int = 30
    page: int = 1
    search_type: SearchType = SearchType.VIDEO
    timeout: int = 25
    max_retries: int = 4
    delay_range: tuple[float, float] = (1.5, 4.0)
    proxy: str | None = None
    verify_ssl: bool = True
    allow_adult: bool = False
    workers: int = 12

class VideoResult(NamedTuple):
    """Structured video search result."""
    title: str
    link: str
    img_url: str | None
    time: str | None
    channel_name: str | None
    channel_link: str | None
    meta: str | None
    score: float = 0.0

# â”€â”€ Enhanced colourised logging setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
init(autoreset=True)
NEON: dict[str, str] = {
    "CYAN": Fore.CYAN,
    "MAGENTA": Fore.MAGENTA,
    "GREEN": Fore.GREEN,
    "YELLOW": Fore.YELLOW,
    "RED": Fore.RED,
    "BLUE": Fore.BLUE,
    "WHITE": Fore.WHITE,
    "BRIGHT": Style.BRIGHT,
    "DIM": Style.DIM,
    "RESET": Style.RESET_ALL,
}

LOG_FMT = (
    f"{NEON['CYAN']}%(asctime)s{NEON['RESET']} - "
    f"{NEON['MAGENTA']}%(levelname)s{NEON['RESET']} - "
    f"{NEON['BLUE']}[%(engine)s]{NEON['RESET']} "
    f"{NEON['GREEN']}%(message)s{NEON['RESET']}"
)

class ContextFilter(logging.Filter):
    """Enhanced context filter with request tracking."""
    def filter(self, record):
        record.engine = getattr(record, "engine", "unknown")
        record.query = getattr(record, "query", "unknown")
        record.request_id = getattr(record, "request_id", "N/A")
        return True

def setup_logging(verbose: bool = False) -> logging.Logger:
    """Configure enhanced logging with rotation."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    handlers: list[logging.Handler] = [
        logging.StreamHandler(sys.stdout)
    ]

    try:
        # Rotating file handler with compression
        file_handler = logging.handlers.RotatingFileHandler(
            log_dir / "video_search.log",
            maxBytes=10_485_760,  # 10MB
            backupCount=5,
            encoding="utf-8"
        )
        file_handler.setFormatter(logging.Formatter(LOG_FMT))
        handlers.append(file_handler)
    except (PermissionError, OSError):
        pass

    logging.basicConfig(
        level=logging.DEBUG if verbose else logging.INFO,
        format=LOG_FMT,
        handlers=handlers
    )

    logger = logging.getLogger(__name__)
    logger.addFilter(ContextFilter())

    # Silence noisy libraries
    for noisy in ("urllib3", "chardet", "requests", "aiohttp", "selenium", "PIL"):
        logging.getLogger(noisy).setLevel(logging.WARNING)

    return logger

logger = setup_logging()

# â”€â”€ Enhanced Cache System â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class CacheManager:
    """Advanced caching with TTL and persistence."""

    def __init__(self, cache_dir: Path = Path(".cache"), ttl: int = 3600):
        self.cache_dir = cache_dir
        self.ttl = ttl
        self.memory_cache: dict[str, tuple[Any, float]] = {}
        self.cache_dir.mkdir(exist_ok=True)
        self.db_path = self.cache_dir / "cache.db"
        self._init_db()
        self._cleanup_expired()

    def _init_db(self):
        """Initialize SQLite cache database."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    value BLOB,
                    expires REAL,
                    hits INTEGER DEFAULT 0
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_expires ON cache(expires)")

    def _cleanup_expired(self):
        """Remove expired cache entries."""
        now = time.time()
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM cache WHERE expires < ?", (now,))
        self.memory_cache = {
            k: v for k, v in self.memory_cache.items()
            if v > now
        }

    def get(self, key: str) -> Any | None:
        """Retrieve from cache if not expired."""
        # Check memory cache first
        if key in self.memory_cache:
            value, expires = self.memory_cache[key]
            if expires > time.time():
                return value
            del self.memory_cache[key]

        # Check persistent cache
        with sqlite3.connect(self.db_path) as conn:
            result = conn.execute(
                "SELECT value, expires FROM cache WHERE key = ?",
                (key,)
            ).fetchone()

            if result:
                value_blob, expires = result
                if expires > time.time():
                    value = pickle.loads(value_blob)
                    self.memory_cache[key] = (value, expires)
                    conn.execute(
                        "UPDATE cache SET hits = hits + 1 WHERE key = ?",
                        (key,)
                    )
                    return value
                conn.execute("DELETE FROM cache WHERE key = ?", (key,))

        return None

    def set(self, key: str, value: Any, ttl: int | None = None):
        """Store in cache with TTL."""
        ttl = ttl or self.ttl
        expires = time.time() + ttl

        # Store in memory
        self.memory_cache[key] = (value, expires)

        # Store persistently
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO cache (key, value, expires) VALUES (?, ?, ?)",
                (key, pickle.dumps(value), expires)
            )

    def invalidate(self, pattern: str | None = None):
        """Invalidate cache entries matching pattern."""
        if pattern:
            # Pattern-based invalidation
            keys_to_remove = [
                k for k in self.memory_cache.keys()
                if re.match(pattern, k)
            ]
            for key in keys_to_remove:
                del self.memory_cache[key]

            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM cache WHERE key LIKE ?", (pattern,))
        else:
            # Clear all
            self.memory_cache.clear()
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM cache")

# â”€â”€ Connection Pool Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ConnectionPoolManager:
    """Manages connection pools with automatic cleanup."""

    def __init__(self, max_pools: int = 10, max_per_pool: int = 30):
        self.max_pools = max_pools
        self.max_per_pool = max_per_pool
        self.pools: dict[str, requests.Session] = {}
        self.pool_stats: dict[str, dict[str, Any]] = defaultdict(
            lambda: {"requests": 0, "errors": 0, "last_used": time.time()}
        )
        self._lock = threading.Lock()
        self._cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)
        self._cleanup_thread.start()

    def get_session(
        self,
        domain: str,
        timeout: int = 25,
        max_retries: int = 4,
        proxy: str | None = None,
        verify_ssl: bool = True
    ) -> requests.Session:
        """Get or create a session for domain."""
        with self._lock:
            if domain not in self.pools:
                if len(self.pools) >= self.max_pools:
                    # Evict least recently used
                    lru_domain = min(
                        self.pool_stats.keys(),
                        key=lambda k: self.pool_stats[k]["last_used"]
                    )
                    self.pools[lru_domain].close()
                    del self.pools[lru_domain]
                    del self.pool_stats[lru_domain]

                self.pools[domain] = self._create_session(
                    timeout, max_retries, proxy, verify_ssl
                )

            self.pool_stats[domain]["last_used"] = time.time()
            self.pool_stats[domain]["requests"] += 1
            return self.pools[domain]

    def _create_session(
        self,
        timeout: int,
        max_retries: int,
        proxy: str | None,
        verify_ssl: bool
    ) -> requests.Session:
        """Create a new configured session."""
        session = requests.Session()

        retries = Retry(
            total=max_retries,
            backoff_factor=2.0,
            backoff_jitter=0.5,
            status_forcelist=(408, 429, 500, 502, 503, 504, 520, 521, 522, 524),
            allowed_methods=frozenset({"GET", "HEAD", "OPTIONS", "POST"}),
            respect_retry_after_header=True,
            raise_on_status=False
        )

        adapter = HTTPAdapter(
            max_retries=retries,
            pool_connections=self.max_per_pool,
            pool_maxsize=self.max_per_pool,
            pool_block=False
        )

        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update(get_realistic_headers(random.choice(REALISTIC_USER_AGENTS)))

        if proxy:
            session.proxies.update({"http": proxy, "https": proxy})

        session.verify = verify_ssl
        session.timeout = timeout

        return session

    def _cleanup_worker(self):
        """Background thread to clean up idle connections."""
        while True:
            time.sleep(60)  # Check every minute
            with self._lock:
                now = time.time()
                idle_threshold = 300  # 5 minutes

                domains_to_remove = [
                    domain for domain, stats in self.pool_stats.items()
                    if now - stats["last_used"] > idle_threshold
                ]

                for domain in domains_to_remove:
                    if domain in self.pools:
                        self.pools[domain].close()
                        del self.pools[domain]
                    del self.pool_stats[domain]

    def close_all(self):
        """Close all connection pools."""
        with self._lock:
            for session in self.pools.values():
                session.close()
            self.pools.clear()
            self.pool_stats.clear()

# â”€â”€ Rate Limiter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RateLimiter:
    """Domain-specific rate limiting with burst support."""

    def __init__(self, default_rps: float = 2.0, burst_size: int = 5):
        self.default_rps = default_rps
        self.burst_size = burst_size
        self.domain_limits: dict[str, float] = {}
        self.request_times: dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        self._lock = threading.Lock()

    def set_limit(self, domain: str, rps: float):
        """Set custom rate limit for domain."""
        self.domain_limits[domain] = rps

    async def acquire(self, url: str):
        """Async rate limit acquisition."""
        domain = urlparse(url).netloc
        await asyncio.get_event_loop().run_in_executor(None, self._acquire_sync, domain)

    def _acquire_sync(self, domain: str):
        """Synchronous rate limit acquisition."""
        with self._lock:
            rps = self.domain_limits.get(domain, self.default_rps)
            min_interval = 1.0 / rps

            now = time.time()
            recent_requests = self.request_times[domain]

            # Remove old requests outside burst window
            burst_window = self.burst_size / rps
            while recent_requests and recent_requests[0] < now - burst_window:
                recent_requests.popleft()

            # Check if we need to wait
            if len(recent_requests) >= self.burst_size:
                oldest = recent_requests[0]
                wait_time = max(0, oldest + burst_window - now)
                if wait_time > 0:
                    time.sleep(wait_time)
                    now = time.time()

            # Check minimum interval
            if recent_requests:
                last_request = recent_requests[-1]
                wait_time = max(0, last_request + min_interval - now)
                if wait_time > 0:
                    time.sleep(wait_time)
                    now = time.time()

            recent_requests.append(now)

# â”€â”€ defaults & constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THUMBNAILS_DIR = Path("downloaded_thumbnails")
VSEARCH_DIR = Path("vsearch_results")
CACHE_DIR = Path(".vsearch_cache")

DEFAULT_ENGINE = "pexels"
DEFAULT_LIMIT = 30
DEFAULT_PAGE = 1
DEFAULT_FORMAT = "html"
DEFAULT_TIMEOUT = 25
DEFAULT_DELAY = (1.5, 4.0)
DEFAULT_MAX_RETRIES = 4
DEFAULT_WORKERS = 12

ADULT_ENGINES = {
    "xhamster", "pornhub", "xvideos", "xnxx", "youjizz", "redtube"
}
ALLOW_ADULT = False

REALISTIC_USER_AGENTS: Sequence[str] = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 "
    "(KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0",
]

def get_realistic_headers(user_agent: str) -> HeadersDict:
    """Generate realistic browser headers."""
    return {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
                  "image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": random.choice([
            "en-US,en;q=0.9",
            "en-GB,en;q=0.9",
            "en-US,en;q=0.8,es;q=0.6"
        ]),
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
        "DNT": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Sec-GPC": "1",
    }

# â”€â”€ ENGINE_MAP (keeping original structure) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENGINE_MAP: dict[str, dict[str, Any]] = {
    # [Keep your original ENGINE_MAP content here - unchanged]
    # This is a placeholder - insert your actual ENGINE_MAP
}

# â”€â”€ Global instances â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cache_manager = CacheManager(CACHE_DIR)
pool_manager = ConnectionPoolManager()
rate_limiter = RateLimiter()

# â”€â”€ Enhanced helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_dir(path: Path, permissions: int = 0o755) -> None:
    """Create directory with proper permissions and error handling."""
    try:
        path.mkdir(parents=True, exist_ok=True, mode=permissions)
    except (PermissionError, OSError) as exc:
        logger.error(f"Failed to create directory {path}: {exc}")
        # Try alternative location
        alt_path = Path(tempfile.gettempdir()) / path.name
        alt_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"Using alternative directory: {alt_path}")
        return alt_path

def enhanced_slugify(text: str, max_length: int = 100) -> str:
    """Create filesystem-safe slug with better Unicode handling."""
    if not text or not isinstance(text, str):
        return f"untitled_{uuid.uuid4().hex[:8]}"

    # Normalize Unicode
    text = unicodedata.normalize("NFKD", text)

    # Convert to ASCII, keeping some common symbols
    text = text.encode("ascii", "ignore").decode("ascii")

    # Remove unsafe characters
    text = re.sub(r'[<>:"/\|?*\x00-\x1f\x7f-\x9f]', "", text)

    # Replace spaces and multiple underscores
    text = re.sub(r"[^\w\s\-.]", "", text, flags=re.UNICODE)
    text = re.sub(r"[-\s]+", "_", text)

    # Clean up
    text = text.strip("._- ")[:max_length]

    if not text:
        text = f"file_{uuid.uuid4().hex[:8]}"

    # Check for reserved Windows names
    reserved = {
        "con", "prn", "aux", "nul", "com1", "com2", "com3", "com4",
        "com5", "com6", "com7", "com8", "com9", "lpt1", "lpt2",
        "lpt3", "lpt4", "lpt5", "lpt6", "lpt7", "lpt8", "lpt9"
    }

    if text.lower() in reserved:
        text = f"file_{text}"

    return text

def validate_url(url: str) -> bool:
    """Validate URL format and safety."""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc]) and result.scheme in ("http", "https")
    except Exception:
        return False

def sanitize_html(text: str) -> str:
    """Enhanced HTML sanitization."""
    if not text:
        return ""

    # Basic HTML escape
    text = html.escape(text, quote=True)

    # Remove any remaining script tags or javascript
    text = re.sub(r"<script.*?</script>", "", text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r"javascript:", "", text, flags=re.IGNORECASE)
    text = re.sub(r"on\w+\s*=", "", text, flags=re.IGNORECASE)

    return text

@contextmanager
def temporary_file(suffix: str = "", prefix: str = "vsearch_") -> Iterator[Path]:
    """Context manager for temporary files with cleanup."""
    fd, path = tempfile.mkstemp(suffix=suffix, prefix=prefix)
    path_obj = Path(path)
    try:
        os.close(fd)
        yield path_obj
    finally:
        with suppress(OSError):
            path_obj.unlink()

def smart_delay_with_jitter(
    delay_range: tuple[float, float],
    last_request_time: float | None = None,
    jitter: float = 0.3,
    adaptive: bool = True
) -> None:
    """Intelligent delay with adaptive timing."""
    now = time.time()
    base = random.uniform(*delay_range)

    # Add jitter
    jitter_amount = base * jitter * random.uniform(-1, 1)
    wait_time = max(0.5, base + jitter_amount)

    # Adaptive delay based on time of day (less delay during off-peak)
    if adaptive:
        hour = datetime.now().hour
        if 2 <= hour <= 6:  # Off-peak hours
            wait_time *= 0.7
        elif 9 <= hour <= 17:  # Peak hours
            wait_time *= 1.2

    if last_request_time:
        elapsed = now - last_request_time
        if elapsed < wait_time:
            time.sleep(wait_time - elapsed)
    else:
        time.sleep(wait_time)

# â”€â”€ Enhanced async helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def aiohttp_session(
    timeout: int = 25,
    connector_limit: int = 100,
    verify_ssl: bool = True
) -> AsyncGenerator[aiohttp.ClientSession, None]:
    """Create managed aiohttp session with retry support."""
    if not ASYNC_AVAILABLE:
        yield None
        return

    timeout_config = ClientTimeout(total=timeout, connect=10, sock_read=10)

    connector = TCPConnector(
        limit=connector_limit,
        limit_per_host=30,
        ttl_dns_cache=300,
        ssl=verify_ssl,
        force_close=False,
        enable_cleanup_closed=True
    )

    retry_options = ExponentialRetry(
        attempts=3,
        start_timeout=1,
        max_timeout=30,
        factor=2.0,
        statuses={429, 500, 502, 503, 504}
    )

    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout_config,
        headers=get_realistic_headers(random.choice(REALISTIC_USER_AGENTS))) as session:
        retry_client = RetryClient(
            client_session=session,
            retry_options=retry_options
        )
        yield retry_client

async def fetch_with_retry(
    session: aiohttp.ClientSession,
    url: str,
    max_retries: int = 3,
    backoff_factor: float = 2.0
) -> str | None:
    """Fetch URL with exponential backoff retry."""
    if not validate_url(url):
        logger.error(f"Invalid URL: {url}")
        return None

    for attempt in range(max_retries):
        try:
            await rate_limiter.acquire(url)

            async with session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                if response.status == 429:
                    retry_after = response.headers.get("Retry-After", 60)
                    await asyncio.sleep(int(retry_after))
                else:
                    logger.warning(f"HTTP {response.status} for {url}")

        except asyncio.TimeoutError:
            logger.warning(f"Timeout on attempt {attempt + 1} for {url}")
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")

        if attempt < max_retries - 1:
            wait_time = backoff_factor ** attempt * random.uniform(0.5, 1.5)
            await asyncio.sleep(wait_time)

    return None

async def download_thumbnail_async(
    session: aiohttp.ClientSession,
    url: str,
    dest_path: Path,
    semaphore: asyncio.Semaphore,
    convert_format: bool = True
) -> bool:
    """Download thumbnail with format conversion support."""
    async with semaphore:
        try:
            await rate_limiter.acquire(url)

            async with session.get(url) as response:
                if response.status != 200:
                    return False

                content = await response.read()

                # Validate content
                if not content or len(content) < 100:
                    return False

                # Optionally convert format
                if convert_format and PIL_AVAILABLE:
                    try:
                        img = Image.open(io.BytesIO(content))

                        # Convert to RGB if necessary
                        if img.mode in ("RGBA", "LA", "P"):
                            rgb_img = Image.new("RGB", img.size, (255, 255, 255))
                            rgb_img.paste(img, mask=img.split()[-1] if img.mode == "RGBA" else None)
                            img = rgb_img

                        # Optimize size
                        max_size = (320, 240)
                        img.thumbnail(max_size, Image.Resampling.LANCZOS)

                        # Save optimized
                        dest_path = dest_path.with_suffix(".jpg")
                        img.save(dest_path, "JPEG", quality=85, optimize=True)
                    except Exception as e:
                        logger.debug(f"Image processing failed: {e}")
                        # Fall back to raw save
                        if ASYNC_AVAILABLE:
                            async with aiofiles.open(dest_path, "wb") as f:
                                await f.write(content)
                        else:
                            dest_path.write_bytes(content)
                else:
                    # Save raw
                    if ASYNC_AVAILABLE:
                        async with aiofiles.open(dest_path, "wb") as f:
                            await f.write(content)
                    else:
                        dest_path.write_bytes(content)

                return dest_path.exists() and dest_path.stat().st_size > 0

        except Exception as e:
            logger.debug(f"Thumbnail download failed for {url}: {e}")
            return False

def robust_download_sync(
    session: requests.Session,
    url: str,
    dest_path: Path,
    chunk_size: int = 8192
) -> bool:
    """Synchronous download with progress tracking."""
    try:
        with session.get(url, stream=True) as response:
            response.raise_for_status()

            total_size = int(response.headers.get("content-length", 0))

            with open(dest_path, "wb") as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

                        # Progress callback could go here
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            # logger.debug(f"Download progress: {progress:.1f}%")

            return dest_path.exists() and dest_path.stat().st_size > 0

    except Exception as e:
        logger.debug(f"Sync download failed for {url}: {e}")
        return False

# â”€â”€ Enhanced Selenium helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SeleniumManager:
    """Manages Selenium WebDriver instances with pooling."""

    def __init__(self, max_drivers: int = 3):
        self.max_drivers = max_drivers
        self.drivers: list[webdriver.Chrome] = []
        self.available: list[webdriver.Chrome] = []
        self._lock = threading.Lock()

    @contextmanager
    def get_driver(self, headless: bool = True) -> Iterator[webdriver.Chrome]:
        """Get a WebDriver from pool or create new."""
        driver = None
        try:
            with self._lock:
                if self.available:
                    driver = self.available.pop()
                elif len(self.drivers) < self.max_drivers:
                    driver = self._create_driver(headless)
                    self.drivers.append(driver)
                else:
                    # Wait for available driver
                    while not self.available:
                        time.sleep(0.1)
                    driver = self.available.pop()

            yield driver

        finally:
            if driver:
                with self._lock:
                    self.available.append(driver)

    def _create_driver(self, headless: bool) -> webdriver.Chrome:
        """Create optimized Chrome WebDriver."""
        options = ChromeOptions()

        if headless:
            options.add_argument("--headless=new")

        # Performance optimizations
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-web-security")
        options.add_argument("--disable-features=VizDisplayCompositor")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

        # Stealth mode
        options.add_argument(f"user-agent={random.choice(REALISTIC_USER_AGENTS)}")

        # Resource blocking for speed
        prefs = {
            "profile.default_content_setting_values": {
                "images": 2,
                "plugins": 2,
                "popups": 2,
                "geolocation": 2,
                "notifications": 2,
                "media_stream": 2,
            }
        }
        options.add_experimental_option("prefs", prefs)

        return webdriver.Chrome(options=options)

    def cleanup(self):
        """Close all drivers."""
        with self._lock:
            for driver in self.drivers:
                try:
                    driver.quit()
                except Exception:
                    pass
            self.drivers.clear()
            self.available.clear()

selenium_manager = SeleniumManager() if SELENIUM_AVAILABLE else None

# â”€â”€ SVG generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_placeholder_svg(
    icon: str = "1F3AC",
    bg_color: str = "#1a1a2e",
    fg_color: str = "#4a4a5e",
    size: int = 100
) -> str:
    """Generate inline base64 SVG placeholder with customization."""
    # Handle hex codes
    if len(icon) > 1 and re.fullmatch(r"[0-9A-Fa-f]{4,6}", icon):
        icon = chr(int(icon, 16))

    safe_icon = sanitize_html(icon)

    svg = f'''<svg xmlns="http://www.w3.org/2000/svg" width="{size}" height="{size}" viewBox="0 0 {size} {size}">
        <rect width="100%" height="100%" fill="{bg_color}"/>
        <text x="50%" y="50%" font-family="system-ui, sans-serif" font-size="{size//3}" 
              fill="{fg_color}" text-anchor="middle" dominant-baseline="middle">
            {safe_icon}
        </text>
    </svg>'''

    return "data:image/svg+xml;base64," + base64.b64encode(svg.encode()).decode()

PLACEHOLDER_THUMB_SVG = generate_placeholder_svg("1F3AC")  # ğŸ¬

# â”€â”€ Enhanced HTML template â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HTML_HEAD = """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        :root {
            --bg-primary: #0a0a0f;
            --bg-secondary: #1a1a2e;
            --bg-card: #16213e;
            --text-primary: #e94560;
            --text-secondary: #f5f5f5;
            --text-muted: #a0a0a0;
            --accent: #0f3460;
            --hover: #e94560;
            --shadow: rgba(0, 0, 0, 0.3);
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: linear-gradient(135deg, var(--bg-primary), var(--bg-secondary));
            color: var(--text-secondary);
            min-height: 100vh;
            line-height: 1.6;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        header {
            text-align: center;
            padding: 3rem 1rem;
            background: linear-gradient(135deg, var(--bg-card), var(--accent));
            border-radius: 1rem;
            margin-bottom: 3rem;
            box-shadow: 0 10px 30px var(--shadow);
        }
        
        h1 {
            font-size: 2.5rem;
            color: var(--text-primary);
            margin-bottom: 1rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
        }
        
        .meta-info {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            margin-top: 1rem;
        }
        
        .meta-info span {
            background: rgba(255, 255, 255, 0.1);
            padding: 0.5rem 1rem;
            border-radius: 2rem;
            font-size: 0.9rem;
        }
        
        .search-box {
            margin: 2rem auto;
            max-width: 600px;
        }
        
        #searchInput {
            width: 100%;
            padding: 1rem;
            font-size: 1rem;
            background: var(--bg-card);
            border: 2px solid var(--accent);
            border-radius: 2rem;
            color: var(--text-secondary);
            transition: all 0.3s ease;
        }
        
        #searchInput:focus {
            outline: none;
            border-color: var(--text-primary);
            box-shadow: 0 0 20px rgba(233, 69, 96, 0.3);
        }
        
        .filters {
            display: flex;
            justify-content: center;
            gap: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
        }
        
        .filter-btn {
            padding: 0.5rem 1rem;
            background: var(--bg-card);
            border: 1px solid var(--accent);
            border-radius: 1rem;
            color: var(--text-secondary);
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .filter-btn:hover,
        .filter-btn.active {
            background: var(--text-primary);
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(233, 69, 96, 0.4);
        }
        
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
            gap: 2rem;
            animation: fadeIn 0.5s ease;
        }
        
        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .card {
            background: var(--bg-card);
            border-radius: 1rem;
            overflow: hidden;
            transition: all 0.3s ease;
            cursor: pointer;
            box-shadow: 0 5px 15px var(--shadow);
            position: relative;
        }
        
        .card:hover {
            transform: translateY(-10px) scale(1.02);
            box-shadow: 0 15px 40px var(--shadow);
        }
        
        .card:hover .thumb img {
            transform: scale(1.1);
        }
        
        .thumb {
            position: relative;
            padding-bottom: 56.25%;
            overflow: hidden;
            background: var(--bg-secondary);
        }
        
        .thumb img {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transition: transform 0.3s ease;
        }
        
        .duration {
            position: absolute;
            bottom: 0.5rem;
            right: 0.5rem;
            background: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 0.2rem 0.5rem;
            border-radius: 0.3rem;
            font-size: 0.8rem;
            font-weight: bold;
        }
        
        .body {
            padding: 1rem;
        }
        
        .title {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
            line-height: 1.4;
        }
        
        .title a {
            color: var(--text-secondary);
            text-decoration: none;
            transition: color 0.3s ease;
        }
        
        .title a:hover {
            color: var(--text-primary);
        }
        
        .meta {
            display: flex;
            flex-direction: column;
            gap: 0.3rem;
            font-size: 0.85rem;
            color: var(--text-muted);
        }
        
        .meta .item {
            display: flex;
            align-items: center;
            gap: 0.3rem;
        }
        
        .meta a {
            color: var(--text-muted);
            text-decoration: none;
            transition: color 0.3s ease;
        }
        
        .meta a:hover {
            color: var(--text-primary);
        }
        
        .loading {
            display: none;
            text-align: center;
            padding: 2rem;
            font-size: 1.2rem;
            color: var(--text-primary);
        }
        
        .loading.active {
            display: block;
        }
        
        .stats {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            background: var(--bg-card);
            padding: 1rem;
            border-radius: 1rem;
            box-shadow: 0 5px 20px var(--shadow);
            font-size: 0.9rem;
            z-index: 100;
        }
        
        @media (max-width: 768px) {
            .grid {
                grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
                gap: 1rem;
            }
            
            h1 {
                font-size: 1.8rem;
            }
            
            .stats {
                bottom: 1rem;
                right: 1rem;
                font-size: 0.8rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ¬ {query} Results</h1>
            <div class="meta-info">
                <span>ğŸ” Engine: {engine}</span>
                <span>ğŸ“Š Results: {count}</span>
                <span>â° Generated: {timestamp}</span>
            </div>
        </header>
        
        <div class="search-box">
            <input type="text" id="searchInput" placeholder="Filter results..." />
        </div>
        
        <div class="filters">
            <button class="filter-btn active" data-filter="all">All</button>
            <button class="filter-btn" data-filter="short">Short (&lt;5min)</button>
            <button class="filter-btn" data-filter="medium">Medium (5-20min)</button>
            <button class="filter-btn" data-filter="long">Long (&gt;20min)</button>
        </div>
        
        <div class="loading">Loading results...</div>
        
        <section class="grid">
"""

HTML_TAIL = """
        </section>
        
        <div class="stats">
            <div>Visible: <span id="visibleCount">0</span></div>
            <div>Total: <span id="totalCount">0</span></div>
        </div>
    </div>
    
    <script>
        document.addEventListener("DOMContentLoaded", () => {
            const grid = document.querySelector(".grid");
            const cards = Array.from(grid.children);
            const searchInput = document.getElementById("searchInput");
            const filterButtons = document.querySelectorAll(".filter-btn");
            const visibleCount = document.getElementById("visibleCount");
            const totalCount = document.getElementById("totalCount");

            totalCount.textContent = cards.length;

            const updateVisibleCount = () => {
                const visible = cards.filter(c => c.style.display !== "none").length;
                visibleCount.textContent = visible;
            };

            const filterCards = () => {
                const searchTerm = searchInput.value.toLowerCase();
                const activeFilter = document.querySelector(".filter-btn.active").dataset.filter;

                cards.forEach(card => {
                    const title = (card.dataset.title || "").toLowerCase();
                    const duration = card.dataset.duration || "";
                    
                    const matchesSearch = title.includes(searchTerm);
                    
                    let matchesFilter = false;
                    if (activeFilter === "all") {
                        matchesFilter = true;
                    } else {
                        const timeParts = duration.split(":").map(Number);
                        const minutes = timeParts.length > 1 ? timeParts[0] : 0;
                        
                        if (activeFilter === "short" && minutes < 5) matchesFilter = true;
                        if (activeFilter === "medium" && minutes >= 5 && minutes <= 20) matchesFilter = true;
                        if (activeFilter === "long" && minutes > 20) matchesFilter = true;
                    }

                    card.style.display = matchesSearch && matchesFilter ? "" : "none";
                });
                updateVisibleCount();
            };

            searchInput.addEventListener("input", filterCards);
            
            filterButtons.forEach(btn => {
                btn.addEventListener("click", () => {
                    filterButtons.forEach(b => b.classList.remove("active"));
                    btn.classList.add("active");
                    filterCards();
                });
            });

            updateVisibleCount();
        });
    </script>
</body>
</html>
"""
if __name__ == "__main__":
    main()
